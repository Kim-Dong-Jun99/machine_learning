{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q-JbrYTGsKF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "DRIVE_PATH = '/content/drive'\n",
        "drive.mount(DRIVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGlMMa-pBVjG"
      },
      "source": [
        "# Libraries Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7cxL8zeBVV9"
      },
      "outputs": [],
      "source": [
        "!pip install pyclustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63w1okUl4zNP"
      },
      "source": [
        "# Libraries Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD8d_pEF4UsQ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Basic libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm \n",
        "import copy\n",
        "from matplotlib.cm import get_cmap\n",
        "\n",
        "\n",
        "# Scaler\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
        "\n",
        "# Encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cluster model\n",
        "from sklearn.cluster import KMeans, DBSCAN, MeanShift, estimate_bandwidth\n",
        "from pyclustering.cluster.clarans import clarans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Timer\n",
        "from multiprocessing import Process, Manager\n",
        "\n",
        "# Visualization\n",
        "from pyclustering.cluster import cluster_visualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8kzNdCf41rl"
      },
      "source": [
        "# Tuning Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eexES5Hd5X9i"
      },
      "source": [
        "\n",
        "> TuningModel - A Basic(Parent) class for hyperparameter tuning\n",
        "\n",
        "*   Public Methods:\n",
        "\n",
        " **Constructor(param)**\n",
        "  - @param **param** - dictionary: parameters for tuning\n",
        "\n",
        " **fit(x)**\n",
        " \n",
        " - @param **x** - np.ndarray: Dataframe values for modeling\n",
        "\n",
        " **check_best()**\n",
        "  - Check the best model with scores\n",
        "\n",
        " **plot_score(title)**\n",
        "  - Plot the average score for each key parameters\n",
        "  - @param **title** - str: Title of plot\n",
        "\n",
        "* Protected Methods\n",
        "\n",
        " **_model_execute(x, params)**\n",
        "  - @param **x** - np.ndarray: dataframe values\n",
        "  - @param **params** - dictionary: chosen parameters in whole parameter\n",
        "\n",
        " **_model_scoring(model, x)**\n",
        "  - @param **model** - Any: Data model built in *_model_execute*\n",
        "  - @param **x** - numpy.ndarray: dataframe values\n",
        "\n",
        " **_is_best(model)**\n",
        "  - @param **model** - Any: Data model built in *_model_execute* compares with the best score\n",
        "\n",
        "*   Members\n",
        "\n",
        " *params*\n",
        "\n",
        " - parameter input from constructor\n",
        "\n",
        " *model_list*\n",
        "\n",
        " - model list, availiable after using **fit**\n",
        "\n",
        " *best_params_*\n",
        "\n",
        " - best model's parameters, availiable after using **fit**\n",
        "\n",
        " *best_score_*\n",
        "\n",
        " - best model's score, availiable after using **fit**\n",
        "\n",
        " *best_model_*\n",
        "\n",
        " - best model, availiable after using **fit**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQUWCxrc4bgs"
      },
      "outputs": [],
      "source": [
        "class TuningModel:\n",
        "    def __init__(self, param: dict, timer=600):\n",
        "        self.params = param\n",
        "        self.model_list = []\n",
        "\n",
        "        self.best_params_ = None\n",
        "        self.best_score_ = None\n",
        "        self.best_model_ = None\n",
        "\n",
        "        self._dataset = None\n",
        "        self.__timer = timer\n",
        "\n",
        "    def _model_execute(self, x, params, result):\n",
        "        pass\n",
        "\n",
        "    def _model_scoring(self, model, x):\n",
        "        pass\n",
        "\n",
        "    def _is_best(self, model):\n",
        "        return self.best_score_ is None or self.best_score_ < model['score']\n",
        "\n",
        "    def check_best(self):\n",
        "        for result in self.model_list:\n",
        "            if self._is_best(result):\n",
        "                self.best_score_ = result['score']\n",
        "                self.best_params_ = result['params']\n",
        "                self.best_model_ = result['model']\n",
        "\n",
        "    def fit(self, x):\n",
        "        if self.params is None:\n",
        "            raise Exception('Invalid parameters input: None', 'ParameterError')\n",
        "\n",
        "        self._dataset = x\n",
        "\n",
        "        # Traversal for whole indices with digits\n",
        "        # By computing the digits of each index\n",
        "        #\n",
        "        # Examples:\n",
        "        #\n",
        "        # Consider three digits (4, 2, 2) - in specific (quadratic, binary, binary)\n",
        "        # We can get series of numbers:\n",
        "        # 0 0 0 = 0(10)\n",
        "        # 0 0 1 = 1\n",
        "        # 0 1 0 = 2\n",
        "        # ...\n",
        "        # 3 0 0 = 12\n",
        "        # 3 0 1 = 13\n",
        "        # 3 1 0 = 14\n",
        "        # 3 1 1 = 15\n",
        "        #\n",
        "        # In this case - we can explain the result number as\n",
        "        # 15 = 3 * (2 * 2) + 1 * (2) + 1\n",
        "        #\n",
        "        # Similarly, we can apply them in traversal\n",
        "        # length = 15 -> get indices (3, 1, 1) for clarans - {'n_cluster', 'num_local', 'max_neighbor'}\n",
        "        # By using this, we visit whole parameters.\n",
        "\n",
        "        length = 1\n",
        "        param_keys = []\n",
        "\n",
        "        # 1. Calc the total multiple of length, and the individual length\n",
        "        for key, value in self.params.items():\n",
        "            length *= len(value)\n",
        "            param_keys.append(key)\n",
        "\n",
        "        if length == 0:\n",
        "            raise Exception('Invalid parameters input: No parameters input', 'ParameterError')\n",
        "\n",
        "        # 2. Loop until the length became 0\n",
        "        while length > 0:\n",
        "            params = {}\n",
        "\n",
        "            temp = length - 1\n",
        "            for i, key in enumerate(param_keys):\n",
        "                # 2-1. Calculate the total multiple of the lower digits\n",
        "                remain = 1\n",
        "                for next_key in param_keys[i + 1:]:\n",
        "                    remain *= len(self.params[next_key])\n",
        "\n",
        "                # 2-2. Compute index (div)\n",
        "                index = temp // remain\n",
        "\n",
        "                # 2-3. Remove current digit number (4 2 2) -> (2 2)\n",
        "                temp -= index * remain\n",
        "                params[key] = self.params[key][index]\n",
        "\n",
        "            return_dict = Manager().dict()\n",
        "\n",
        "            process = Process(target=self._model_execute, args=(x, params, return_dict))\n",
        "            process.start()\n",
        "            process.join(timeout=self.__timer)\n",
        "            process.terminate()\n",
        "\n",
        "            length -= 1\n",
        "\n",
        "            if len(return_dict) == 0:\n",
        "                warnings.warn(f'Timeout from {self}: Model with {params} exceed {self.__timer}. Model ignored.',\n",
        "                              UserWarning)\n",
        "                continue\n",
        "\n",
        "            model = return_dict['return']\n",
        "\n",
        "            self.model_list.append({\n",
        "                'params': params,\n",
        "                'model': model,\n",
        "                'score': self._model_scoring(model, x)\n",
        "            })\n",
        "\n",
        "        self.check_best()\n",
        "\n",
        "    def plot_score(self, title: str):\n",
        "        fig, axs = plt.subplots(3, (len(self.params) + 2) // 3, figsize=(10, 10), constrained_layout=True)\n",
        "        plt.suptitle(title)\n",
        "\n",
        "        x = 0\n",
        "        for key, values in self.params.items():\n",
        "            value_list, score_list = [], []\n",
        "\n",
        "            for value in values:\n",
        "                avg_score, quantity = 0, 0\n",
        "                for model in self.model_list:\n",
        "                    if model['params'][key] == value:\n",
        "                        avg_score += model['score']\n",
        "                        quantity += 1\n",
        "\n",
        "                if quantity == 0:\n",
        "                    continue\n",
        "\n",
        "                value_list.append(value)\n",
        "                score_list.append(avg_score / quantity)\n",
        "\n",
        "            print(key)\n",
        "\n",
        "            axs[x].set_title(f'Average score based on the value of {key}')\n",
        "            axs[x].plot(value_list, score_list)\n",
        "            axs[x].set_xlabel(str(key))\n",
        "            axs[x].set_ylabel('score')\n",
        "\n",
        "            x += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXSQVXjcnJBf"
      },
      "source": [
        "> ClaransTune - Tuning class for pyclustering.cluster.clarans.clarans\n",
        "- uses **silhouette coefficient** for scoring\n",
        "- **Constructor(method, param)**\n",
        "  - @param **method**: matrice calculation method - 'euclidean', 'manhattan', .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY6n_OpbnI1q"
      },
      "outputs": [],
      "source": [
        "class ClaransTune(TuningModel):\n",
        "    def __init__(self, method: str, param: dict):\n",
        "        super().__init__(param)\n",
        "        self.__metric = method\n",
        "\n",
        "    def _model_execute(self, x, params, return_dict):\n",
        "        # Order dictionary to tuple for parameter input\n",
        "        param_input = (params['number_clusters'], params['numlocal'], params['maxneighbor'])\n",
        "\n",
        "        # Input it as parameter\n",
        "        model = clarans(x, *param_input)\n",
        "        model.process()\n",
        "\n",
        "        return_dict['return'] = model\n",
        "\n",
        "    def _model_scoring(self, model: clarans, x):\n",
        "        # Match the form of clusters [[cluster1], [cluster2], ...] -> [c1, c2, c1, ...]\n",
        "        y = np.zeros(x.size // x[0].size)\n",
        "        for i in range(0, len(model.get_clusters())):\n",
        "            for index in model.get_clusters()[i]:\n",
        "                y[index] = i + 1\n",
        "\n",
        "        return silhouette_score(x, y, metric=self.__metric)\n",
        "\n",
        "    def plot_score(self, title):\n",
        "        super.plot_score(self, title)\n",
        "        visualizer = cluster_visualizer(10, 4)\n",
        "\n",
        "    def plot_score(self, title: str):\n",
        "        # 4+Dimensional cannot be visualized\n",
        "        if 0 < self._dataset.shape[0] < 4:\n",
        "            visualizer = cluster_visualizer(10, 4)  # 시각적 그래프\n",
        "\n",
        "            # for result in self.model_list: - Too many models, print the best one only.\n",
        "            model = self.best_model_\n",
        "\n",
        "            # Obtain medoid, cluster from model\n",
        "            medoids = model.get_medoids()\n",
        "            clusters = model.get_clusters()\n",
        "\n",
        "            k = self.best_params_['number_clusters']\n",
        "\n",
        "            visualizer.append_clusters(clusters, self._dataset, k - 3)  # cluster 시각화\n",
        "            visualizer.append_cluster(medoids, self._dataset, k - 3, marker='x')  # center 좌표 시각화\n",
        "            visualizer.set_canvas_title(text=f'Clarans Cluster : {k}', canvas=k - 3)\n",
        "            visualizer.show(figure=plt.figure(figsize=(10,10)))\n",
        "\n",
        "        super(ClaransTune, self).plot_score(title)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YGhQ-bq7-ed"
      },
      "source": [
        "Other clusters\n",
        "> DbscanTune - Tuning class for sklearn.cluster.DBSCAN\n",
        "\n",
        "> KMeansTune - Tuning class for sklearn.cluster.KMeans\n",
        "\n",
        "> GMMTune - Tuning class for sklearn.mixture.GaussianMixture\n",
        "\n",
        "> MeanShift - Tuning class for sklearn.cluster.MeanShift\n",
        "\n",
        "- uses **silhouette coefficient** for scoring\n",
        "- **Constructor(method, param)**\n",
        "  - @param **method**: matrice calculation method - 'euclidean', 'manhattan', ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrLp1zh_5C8x"
      },
      "outputs": [],
      "source": [
        "class DbscanTune(TuningModel):\n",
        "    def __init__(self, method: str, param: dict):\n",
        "        super().__init__(param)\n",
        "        self.__metric = method\n",
        "\n",
        "    def _model_execute(self, x, params, return_dict):\n",
        "        model = DBSCAN(**params) # send input parameter as keyword to constructor\n",
        "        model.fit(x)\n",
        "\n",
        "        return_dict['return'] = model\n",
        "\n",
        "    def _model_scoring(self, model: DBSCAN, x):\n",
        "      # Error handling: Number of cluster < 2\n",
        "      # DBSCAN does not clearify the number of clusters first\n",
        "      # When we use silhouette score, we can get case of 1 cluster model\n",
        "      # Return -1 (worst score) when no clustering\n",
        "        try:\n",
        "            score = silhouette_score(x, model.labels_, metric=self.__metric)\n",
        "        except ValueError:\n",
        "            return -1.0\n",
        "\n",
        "        return score\n",
        "\n",
        "\n",
        "class KMeansTune(TuningModel):\n",
        "    def __init__(self, method: str, param: dict):\n",
        "        super().__init__(param)\n",
        "        self.__metric = method\n",
        "\n",
        "    def _model_execute(self, x, params, return_dict):\n",
        "        model = KMeans(**params)\n",
        "        model.fit(x)\n",
        "\n",
        "        return_dict['return'] = model\n",
        "\n",
        "    def _model_scoring(self, model: KMeans, x):\n",
        "        return silhouette_score(x, model.labels_, metric=self.__metric)\n",
        "\n",
        "\n",
        "class GMMTune(TuningModel):\n",
        "    def __init__(self, method: str, param: dict):\n",
        "        super().__init__(param)\n",
        "        self.__metric = method\n",
        "\n",
        "    def _model_execute(self, x, params, return_dict):\n",
        "        model = GaussianMixture(**params)\n",
        "        model.fit(x)\n",
        "\n",
        "        return_dict['return'] = model\n",
        "\n",
        "    def _model_scoring(self, model: GaussianMixture, x):\n",
        "        return silhouette_score(x, model.predict(x), metric=self.__metric)\n",
        "\n",
        "\n",
        "class MeanShiftTune(TuningModel):\n",
        "    def __init__(self, method: str, param: dict):\n",
        "        super().__init__(param)\n",
        "        self.__metric = method\n",
        "\n",
        "    def _model_execute(self, x, params, return_dict):\n",
        "      model=MeanShift(**params)\n",
        "      model.fit(x)\n",
        "\n",
        "      return_dict['return'] = model\n",
        "\n",
        "    def _model_scoring(self, model: MeanShift, x):\n",
        "        try:\n",
        "            score = silhouette_score(x, model.labels_, metric=self.__metric)\n",
        "        except ValueError:\n",
        "            return -1.0\n",
        "\n",
        "        return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZXguMXW9F4E"
      },
      "source": [
        "# AutoML - A major function for one operation\n",
        "\n",
        "- Only supports K-Means, K-Medoids(CLARANS), DBSCAN, Gaussian-Mixture, and MeanShift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f1DRZFg5BUE"
      },
      "outputs": [],
      "source": [
        "# Declare supported_model\n",
        "supported_model = {\n",
        "    KMeans: KMeansTune,\n",
        "    clarans: ClaransTune,\n",
        "    DBSCAN: DbscanTune,\n",
        "    GaussianMixture: GMMTune,\n",
        "    MeanShift: MeanShiftTune\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVH5dtJx9W8I"
      },
      "source": [
        "> Major function\n",
        "\n",
        "*  Parameters\n",
        "\n",
        " @param **x** - pandas.DataFrame: A dataframe to use\n",
        "\n",
        " @param **kwargs**: keyword arguments for clustering\n",
        "\n",
        "*  **Argument kwargs contains**:\n",
        " \n",
        " *scaler*: Scaler **types** to use - the modeling is executed with Default + scaled datasets\n",
        "\n",
        " *cluster*: Clusters to use - it is **dictionary** that is having the **Cluster Type** as key, and **Its Hyperparametrs** as value.\n",
        "\n",
        "* **Returns**: *list(tuple, TuningModel)*\n",
        "\n",
        "  A list of tuning models contains the proceed *TuningModel* and calculated scores.\n",
        "  \n",
        "  Element *tuple* contains (its whole parameter, cluster type, scaler type).\n",
        "\n",
        "\n",
        "    scalar_list = {StandardScaler, MinMaxScaler}\n",
        "    cluster_list = {\n",
        "        KMeans: {\n",
        "            'n_clusters': [2, 3, 4],\n",
        "            'init': ['k-means++', 'random']\n",
        "        },\n",
        "        clarans: {\n",
        "            'number_clusters': [2, 3, 4],\n",
        "            'numlocal': [2, 4, 6],\n",
        "            'maxneighbor': [3, 5, 7]\n",
        "        },\n",
        "        DBSCAN: {\n",
        "            'eps': [0.01, 0.05, 0.1],\n",
        "            'min_samples': range(2, 6)\n",
        "        },\n",
        "        GaussianMixture: {\n",
        "            'n_components': [2, 3, 4]\n",
        "        },\n",
        "        MeanShift:{\n",
        "         'bandwidth' : [0.8,1.6,3.0]\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTs0-bJX4x-e"
      },
      "outputs": [],
      "source": [
        "def major_function(x: pd.DataFrame, **kwargs):\n",
        "    # Dataframe list with name\n",
        "    dataframes = {None: x.values}\n",
        "\n",
        "    # Scalers: Set\n",
        "    scalers = kwargs.get('scaler')\n",
        "\n",
        "    # Clusters: Dict (Cluster Type: Parameters)\n",
        "    clusters = kwargs.get('cluster')\n",
        "\n",
        "    # Metric methods: Set (str)\n",
        "    methods = kwargs.get('metric')\n",
        "\n",
        "    if clusters is None or len(clusters) == 0:\n",
        "        raise Exception('InputError: No cluster input')\n",
        "\n",
        "    if scalers is not None:\n",
        "        for scaler in scalers:\n",
        "            # Append scaled dataset with their type\n",
        "            dataframes[scaler] = scaler().fit_transform(x)\n",
        "\n",
        "    output = []\n",
        "\n",
        "    for method in methods:\n",
        "        for cluster, param in clusters.items():\n",
        "            if not cluster in supported_model.keys():\n",
        "                warnings.warn(f'Model {cluster} is not supported.', UserWarning)\n",
        "                continue\n",
        "\n",
        "            for key, value in dataframes.items():\n",
        "                model = supported_model[cluster](method, param)\n",
        "\n",
        "                if model is not None:\n",
        "                    model.fit(value)\n",
        "                    output.append((f'{key},{cluster},{method}', model))\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZVmqWYf_ICY"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ww94bNg_i2h"
      },
      "outputs": [],
      "source": [
        "# an alternative dataframe reads: open file in runtime\n",
        "\n",
        "#ROOT_PATH = '/content'\n",
        "#DRIVE_PATH = '/drive'\n",
        "#FILE_NAME = 'housing.csv'\n",
        "\n",
        "#df = pd.read_csv(f'{ROOT_PATH}/{FILE_NAME}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7oW3QmF9h6p"
      },
      "outputs": [],
      "source": [
        "# Import dataset\n",
        "FILE_PATH = \"/MyDrive/machine_learning/data/\"\n",
        "FILE_NAME = \"housing.csv\"\n",
        "\n",
        "df = pd.read_csv(f\"{DRIVE_PATH}/{FILE_PATH}/{FILE_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnuGPWg_AA6z"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn9wq_OVACx9"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox8NfFX4ADok"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLJPWf4USw4l"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AInryMAGtG"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4iBL6mMb4z5"
      },
      "outputs": [],
      "source": [
        "# Copy data\n",
        "df_src = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onMGsZdeAIk0"
      },
      "outputs": [],
      "source": [
        "df = df_src.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXh90dl6_-sr"
      },
      "outputs": [],
      "source": [
        "# Split median_house_value (for clustering)\n",
        "df_median = df['median_house_value']\n",
        "\n",
        "# Drop median_house_df\n",
        "df.drop(['median_house_value'], axis=1, inplace=True)\n",
        "\n",
        "# Drop NaN any with how=any\n",
        "df.dropna(how='any', inplace=True)\n",
        "\n",
        "# Check splitted one\n",
        "df_median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gduxYFzVAS6n"
      },
      "outputs": [],
      "source": [
        "# Check NaN again\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6ePt5XLARNM"
      },
      "outputs": [],
      "source": [
        "# Check categorical data counts\n",
        "df['ocean_proximity'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcgH7JUSAUVS"
      },
      "outputs": [],
      "source": [
        "# Method 1 - LabelEncoder, switch the categorical feature into numeric one\n",
        "encoder = LabelEncoder()\n",
        "df['ocean_proximity'] = encoder.fit_transform(df['ocean_proximity'])\n",
        "print(df['ocean_proximity'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC46tUhxAWvu"
      },
      "outputs": [],
      "source": [
        "# Display histogram of features\n",
        "df.hist(bins=50, figsize=(20, 15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLL6ftkgAxjc"
      },
      "source": [
        "# Data Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXvxuIDec8OW"
      },
      "source": [
        "> Sample Clusterings - do it without AutoML\n",
        "- Arbitrary chosen parameters\n",
        "- Standard-scaled data\n",
        "- Silhouette index for scoring (euclidean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycsGnt4lq3m3"
      },
      "source": [
        "> K-Means (k=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTNEtOduiH6q"
      },
      "outputs": [],
      "source": [
        "X = StandardScaler().fit_transform(df)\n",
        "model = KMeans(n_clusters=4, random_state=1)\n",
        "\n",
        "# Do Analysis\n",
        "cluster_labels = model.fit_predict(X)\n",
        "\n",
        "# Get info\n",
        "#cluster_info = pd.DataFrame(cluster_labels).drop_duplicates().to_numpy().flatten()\n",
        "\n",
        "# Make dataframe for each cluster\n",
        "#clusters_df = []\n",
        "#for i in range(0, len(cluster_info)):\n",
        "#  clusters_df.append(pd.DataFrame(columns=df.columns))\n",
        "#for i in range(0, len(cluster_labels)):\n",
        "#  clusters_df[cluster_labels[i]] = clusters_df[cluster_labels[i]].append(df.iloc[i, :])\n",
        "\n",
        "# Print score\n",
        "print(\"Model score: \", silhouette_score(X, cluster_labels))\n",
        "\n",
        "# Plots\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "plt.scatter(df.loc[:, 'longitude'], df.loc[:, 'latitude'], c=cluster_labels, s=50, cmap='viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24yIufAZvisa"
      },
      "source": [
        "> DBSCAN (eps=0.5, minSamples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXiHeH2trChg"
      },
      "outputs": [],
      "source": [
        "X = StandardScaler().fit_transform(df)\n",
        "model = DBSCAN(eps=0.5, min_samples=5)\n",
        "\n",
        "# Do analysis\n",
        "cluster_labels = model.fit_predict(X)\n",
        "\n",
        "# Get info\n",
        "#cluster_info = pd.DataFrame(cluster_labels).drop_duplicates().to_numpy().flatten()\n",
        "\n",
        "# Make dataframe for each cluster\n",
        "#clusters_df = []\n",
        "#for i in range(0, len(cluster_info)):\n",
        "#  clusters_df.append(pd.DataFrame(columns=df.columns))\n",
        "#for i in range(0, len(cluster_labels)):\n",
        "#  clusters_df[cluster_labels[i]] = clusters_df[cluster_labels[i]].append(df.iloc[i, :])\n",
        "\n",
        "# Print score\n",
        "print(\"Model score: \", silhouette_score(X, cluster_labels))\n",
        "\n",
        "# Plots\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "plt.scatter(df.loc[:, 'longitude'], df.loc[:, 'latitude'], c=cluster_labels, s=50, cmap='viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PenBae1owdYD"
      },
      "source": [
        "> GaussianMixture (n_components=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi0CA44BvsAd"
      },
      "outputs": [],
      "source": [
        "X = StandardScaler().fit_transform(df)\n",
        "model = GaussianMixture(n_components=4, random_state=10)\n",
        "\n",
        "# Do analysis\n",
        "cluster_labels = model.fit_predict(X)\n",
        "\n",
        "# Get info\n",
        "#cluster_info = pd.DataFrame(cluster_labels).drop_duplicates().to_numpy().flatten()\n",
        "\n",
        "# Make dataframe for each cluster\n",
        "#clusters_df = []\n",
        "#for i in range(0, len(cluster_info)):\n",
        "#  clusters_df.append(pd.DataFrame(columns=df.columns))\n",
        "#for i in range(0, len(cluster_labels)):\n",
        "#  clusters_df[cluster_labels[i]] = clusters_df[cluster_labels[i]].append(df.iloc[i, :])\n",
        "\n",
        "# Print score\n",
        "print(\"Model score: \", silhouette_score(X, cluster_labels))\n",
        "\n",
        "# Plots\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "plt.scatter(df.loc[:, 'longitude'], df.loc[:, 'latitude'], c=cluster_labels, s=50, cmap='viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQp7V6gvzLRF"
      },
      "source": [
        "> MeanShift (bandwidth=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZGD0BydxDsk"
      },
      "outputs": [],
      "source": [
        "X = StandardScaler().fit_transform(df)\n",
        "model = MeanShift(bandwidth=0.6)\n",
        "\n",
        "# Do analysis\n",
        "cluster_labels = model.fit_predict(X)\n",
        "\n",
        "# Get info\n",
        "#cluster_info = pd.DataFrame(cluster_labels).drop_duplicates().to_numpy().flatten()\n",
        "\n",
        "# Make dataframe for each cluster\n",
        "#clusters_df = []\n",
        "#for i in range(0, len(cluster_info)):\n",
        "#  clusters_df.append(pd.DataFrame(columns=df.columns))\n",
        "#for i in range(0, len(cluster_labels)):\n",
        "#  clusters_df[cluster_labels[i]] = clusters_df[cluster_labels[i]].append(df.iloc[i, :])\n",
        "\n",
        "# Print score\n",
        "print(\"Model score: \",silhouette_score(X, cluster_labels))\n",
        "\n",
        "# Plots\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "plt.scatter(df.loc[:, 'longitude'], df.loc[:, 'latitude'], c=cluster_labels, s=50, cmap='viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoBQF9gRNhLA"
      },
      "source": [
        "> CLARANS (n_clusters=4, numlocal=1, maxneighbor=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X0sg2hHNgBP"
      },
      "outputs": [],
      "source": [
        "X = StandardScaler().fit_transform(df)\n",
        "\n",
        "# Do analysis\n",
        "model = clarans(X.tolist(), number_clusters=4, numlocal=1, maxneighbor=1)\n",
        "model.process()\n",
        "\n",
        "# Get info\n",
        "cluster_labels = np.zeros(X.size // X[0].size)\n",
        "\n",
        "for i in range(0, len(model.get_clusters())):\n",
        "  for index in model.get_clusters()[i]:\n",
        "    cluster_labels[index] = i + 1\n",
        "\n",
        "#cluster_info = pd.DataFrame(cluster_labels).drop_duplicates().to_numpy().flatten()\n",
        "\n",
        "# Make dataframe for each cluster\n",
        "#clusters_df = []\n",
        "#for i in range(0, len(cluster_info)):\n",
        "#  clusters_df.append(pd.DataFrame(columns=df.columns))\n",
        "#for i in range(0, len(cluster_labels)):\n",
        "#  clusters_df[int(cluster_labels[i])-1] = clusters_df[int(cluster_labels[i])-1].append(df.iloc[i, :])\n",
        "\n",
        "# Print score\n",
        "print(\"Model score: \",silhouette_score(X, cluster_labels))\n",
        "\n",
        "# Plots\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "plt.scatter(df.loc[:, 'longitude'], df.loc[:, 'latitude'], c=cluster_labels, s=50, cmap='viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdAg_-b6Gj_s"
      },
      "source": [
        "> Processes with AutoML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8DQWdxqAPHH"
      },
      "outputs": [],
      "source": [
        "k_range = range(2, 13)\n",
        "\n",
        "scaler_list = {StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler}\n",
        "cluster_list = {\n",
        "    KMeans: {\n",
        "        'n_clusters': k_range,\n",
        "        'init': ['k-means++', 'random'],\n",
        "        'random_state': [1]\n",
        "    },\n",
        "    clarans: {\n",
        "       'number_clusters': k_range,\n",
        "       'numlocal': [1, 2, 3],\n",
        "       'maxneighbor': [1, 2, 3]\n",
        "    },\n",
        "    DBSCAN: {\n",
        "        'eps': [1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1],\n",
        "        'min_samples': range(2, 6)\n",
        "    },\n",
        "    GaussianMixture: {\n",
        "        'n_components': k_range,\n",
        "        'random_state': [1]\n",
        "    },\n",
        "    MeanShift:{\n",
        "        'bandwidth' : [8e-1, 16e-1, 3]\n",
        "    }\n",
        "}\n",
        "method_list = {'euclidean', 'manhattan'}\n",
        "\n",
        "output = major_function(df, scaler=scaler_list, cluster=cluster_list, metric=method_list)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAT4L5nPBz8G"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO_VY-ZqB1GC"
      },
      "outputs": [],
      "source": [
        "output.sort(key=lambda i:i[1].best_score_, reverse=True)\n",
        "for result in output:\n",
        "  print(f'TuningModel: {result[0]}: {result[1]}')\n",
        "  print(f'Best parameters: {result[1].best_params_}')\n",
        "  print(f'Best Model: {result[1].best_model_}')\n",
        "  print(f'Best Score: {result[1].best_score_}', end='\\n\\n')\n",
        "  \n",
        "  result[1].plot_score(f'Plot of {result[0]}')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUEgmX93Hw6a"
      },
      "source": [
        "# Comparison\n",
        "- Creating cluster by median_house_value attribute to compare with AutoML function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oljw82YKHy8Z"
      },
      "outputs": [],
      "source": [
        "def result_vs_cluster_with_median_house_value():\n",
        "  df_with_median = df_median.copy()\n",
        "\n",
        "  # Drop any with how=any\n",
        "  df_with_median.dropna(how='any', inplace=True)\n",
        "  # Create new cluster by median_house_value attribute\n",
        "  df_with_median.reset_index(drop=True, inplace=True)\n",
        "  n_df = df_with_median.sort_values()\n",
        "  n_df.reset_index(drop=True,inplace=True)\n",
        "  n = 20433\n",
        "  q1 = int((0.3333 * (n + 1)) - 1)\n",
        "  q2 = int((0.6666 * (n + 1)) - 1)\n",
        "  df['target'] = np.nan\n",
        "  print(q1)\n",
        "  print(q2)\n",
        "  print(n_df[q1])\n",
        "  print(n_df[q2])\n",
        "  df_with_median['target'] = np.nan\n",
        "  df_with_median.loc[df_with_median['median_house_value'] < 141300, 'target'] = 'cheap'\n",
        "  df_with_median.loc[df_with_median['target'] != ('cheap'), 'target'] = 'normal'\n",
        "  df_with_median.loc[df_with_median['median_house_value'] > 230200, 'target'] = 'expensive'\n",
        "  df_with_median.drop(['median_house_value'], axis=1, inplace=True)\n",
        "\n",
        "  # Encode the categorical datas\n",
        "  df_with_median['ocean_proximity'] = encoder.fit_transform(df_with_median['ocean_proximity'])\n",
        "  df_with_median['target'] = encoder.fit_transform(df_with_median['target'])\n",
        "  print(df_with_median['ocean_proximity'].value_counts())\n",
        "  print(df_with_median['target'].value_counts())\n",
        "  # Scale the dataset\n",
        "  columns = df_with_median.columns\n",
        "  data = MaxAbsScaler().fit_transform(df_with_median)\n",
        "  data = pd.DataFrame(data,columns = columns)\n",
        "  ids = data['target']\n",
        "  # Calculate silhouette score\n",
        "  score = silhouette_score(data, ids)\n",
        "  print(\"silhouette score for clustering with median house value %f\"%(score))\n",
        "  # Plot the clustering result and compare it with result of AutoML function\n",
        "  # Plotting clustering of median house value\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.subplot(2,1,1)\n",
        "  plt.tight_layout()\n",
        "  plt.title(\"clustering with median house value\")\n",
        "  plt.xlabel('longitude')\n",
        "  plt.ylabel('latitude')\n",
        "  plt.scatter(data['longitude'],data['latitude'], c=ids,cmap = get_cmap('plasma'))\n",
        "\n",
        "  # Plotting clustering of AutoML function\n",
        "  df_original = df_src.copy()\n",
        "  df_original.dropna(how='any', inplace=True)\n",
        "  df_original['ocean_proximity'] = encoder.fit_transform(df_original['ocean_proximity'])\n",
        "  # Scale the dataset\n",
        "  columns = df_original.columns\n",
        "  data = MaxAbsScaler().fit_transform(df_original)\n",
        "  data = pd.DataFrame(data, columns = columns)\n",
        "  \n",
        "  ids = output[0][1].best_model_.fit_predict(data)\n",
        "  score = silhouette_score(data, ids)\n",
        "  print(\"silhouette score for clustering with AutoML function %f\"%(score))\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.tight_layout()\n",
        "  plt.title(\"clustering with AutoML\")\n",
        "  plt.xlabel('longitude')\n",
        "  plt.ylabel('latitude')\n",
        "  plt.scatter(data['longitude'],data['latitude'], c=ids,cmap = get_cmap('plasma'))\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq8dAhPjH1-C"
      },
      "outputs": [],
      "source": [
        "result_vs_cluster_with_median_house_value()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sygcqNyopQ9D"
      },
      "source": [
        "# Clustering#2 (sub dataset)\n",
        "> Correlation using Matrix Heatmap - drop 4 low correlationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1qdkbGrg2iT"
      },
      "outputs": [],
      "source": [
        "df = df_src.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iaaKQD-j-lD"
      },
      "source": [
        "> Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qROmrmCHpPQe"
      },
      "outputs": [],
      "source": [
        "# Drop any with how=any\n",
        "df.dropna(how='any', inplace=True)\n",
        "\n",
        "# Method 1 - LabelEncoder, switch the categorical feature into numeric one\n",
        "encoder = LabelEncoder()\n",
        "df['ocean_proximity'] = encoder.fit_transform(df['ocean_proximity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRnNi5lQcjjs"
      },
      "outputs": [],
      "source": [
        "# Display heatmap for feature engineering\n",
        "fig, ax = plt.subplots(figsize=(15, 15))\n",
        "ax = sns.heatmap(df.corr(), annot=True, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd64Iuq8jRVB"
      },
      "outputs": [],
      "source": [
        "# Drop 4 attributes with low correlation coefficient with median_house_value.\n",
        "drop_target = df.corr()['median_house_value'].sort_values().iloc[:4].index\n",
        "print(drop_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKcUZaIfjZJ_"
      },
      "outputs": [],
      "source": [
        "df.drop(list(drop_target), axis=1, inplace=True)\n",
        "\n",
        "# Drop median_house_value (for clustering)\n",
        "df.drop(['median_house_value'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svhp8pdQj3dS"
      },
      "source": [
        "> Data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXVUeyKY_tjn"
      },
      "outputs": [],
      "source": [
        "# Same conditions of first tried, but engineered featuers (df) used\n",
        "output = major_function(df, scaler=scaler_list, cluster=cluster_list, metric=method_list)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shgixrrt_6Hv"
      },
      "source": [
        "> Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE1edknL_3fR"
      },
      "outputs": [],
      "source": [
        "output.sort(key = lambda i:i[1].best_score_, reverse=True)\n",
        "for out in output:\n",
        "  print(out[0])\n",
        "  print(out[1])\n",
        "  print(out[1].best_params_)\n",
        "  print(out[1].best_model_)\n",
        "  print(out[1].best_score_)\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}