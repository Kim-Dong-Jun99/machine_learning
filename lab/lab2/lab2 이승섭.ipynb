{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_K01k4latMMI",
    "outputId": "d967543c-72b6-4ea6-f4c4-072485f24612"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyclustering\n",
      "  Downloading pyclustering-0.10.1.2.tar.gz (2.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 2.6 MB 34.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pyclustering) (1.7.3)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from pyclustering) (3.2.2)\n",
      "Requirement already satisfied: numpy>=1.15.2 in /usr/local/lib/python3.7/dist-packages (from pyclustering) (1.21.6)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from pyclustering) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->pyclustering) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->pyclustering) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->pyclustering) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->pyclustering) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->pyclustering) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->pyclustering) (1.15.0)\n",
      "Building wheels for collected packages: pyclustering\n",
      "  Building wheel for pyclustering (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyclustering: filename=pyclustering-0.10.1.2-py3-none-any.whl size=2395122 sha256=6a1af3ae814d237890b0ccc6784ef6f3f44c146cdfcb4e7e5ff730c57b063147\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/87/6b/1e0568b5ba9dc6518a25338bae90bd8392f35206bb90bb10f1\n",
      "Successfully built pyclustering\n",
      "Installing collected packages: pyclustering\n",
      "Successfully installed pyclustering-0.10.1.2\n"
     ]
    }
   ],
   "source": [
    "# pip install pyclustering"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from numpy.random.mtrand import randint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from pyclustering.cluster.clarans import clarans as CLARANS\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from itertools import combinations\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Change CLARANS result to ScikitLearn result\n",
    "def clarans_label_converter(labels):\n",
    "  total_len = 0\n",
    "  for k in range(0, len(labels)):\n",
    "    total_len += len(labels[k])\n",
    "\n",
    "  outList = np.empty((total_len), dtype=int)\n",
    "  cluster_number = 0\n",
    "  for k in range(0, len(labels)):\n",
    "    for l in range(0, len(labels[k])):\n",
    "      outList[labels[k][l]] = cluster_number\n",
    "    cluster_number += 1\n",
    "  return outList\n",
    "\n",
    "def findKMeans(X,cluster_k_value,cur_scaler):\n",
    "  global best_score,best_k_,best_scaler,best_model,best_score,labels_\n",
    "  labels = None\n",
    "  model=KMeans(n_clusters = cluster_k_value)\n",
    "  labels = model.fit_predict(X)\n",
    "  score_result = silhouette_score(X, labels)\n",
    "  # if mean value of scores are bigger than max variable,\n",
    "  # update new options(model, scaler, k) to best options\n",
    "  if best_score < score_result:\n",
    "    best_score = score_result\n",
    "    best_scaler = cur_scaler\n",
    "    best_model = KMeans()\n",
    "    best_k_ = cluster_k_value\n",
    "    labels_ = copy.deepcopy(labels)\n",
    "\n",
    "def findGMM(X,cluster_k_value,cur_scaler):\n",
    "  global best_score,best_k_,best_scaler,best_model,best_score,labels_\n",
    "  labels = None\n",
    "  model=GaussianMixture(n_components = cluster_k_value)\n",
    "  labels = model.fit_predict(X)\n",
    "  score_result = silhouette_score(X, labels)\n",
    "  # if mean value of scores are bigger than max variable,\n",
    "  # update new options(model, scaler, k) to best options\n",
    "  if best_score < score_result:\n",
    "    best_score = score_result\n",
    "    best_scaler = cur_scaler\n",
    "    best_model = GaussianMixture()\n",
    "    best_k_ = cluster_k_value\n",
    "    labels_ = copy.deepcopy(labels)\n",
    "\n",
    "def findCLARANS(X,cluster_k_value,cur_scaler):\n",
    "  global best_score,best_k_,best_scaler,best_model,best_score,labels_\n",
    "  labels = None\n",
    "  model=CLARANS(data=X.tolist(),number_clusters=cluster_k_value, numlocal=2, maxneighbor=3)\n",
    "  model.process()\n",
    "  clarans_label = model.get_clusters()\n",
    "  labels = clarans_label_converter(labels=clarans_label)\n",
    "  score_result = silhouette_score(X, labels)\n",
    "  # if mean value of scores are bigger than max variable,\n",
    "  # update new options(model, scaler, k) to best options\n",
    "  if best_score < score_result:\n",
    "    best_score = score_result\n",
    "    best_scaler = cur_scaler\n",
    "    best_model = CLARANS(data=X.tolist(),number_clusters=cluster_k_value, numlocal=2, maxneighbor=3)\n",
    "    best_k_ = cluster_k_value\n",
    "    labels_ = copy.deepcopy(labels)\n",
    "\n",
    "def findDBSCAN(X,cur_scaler):\n",
    "  global best_score,best_k_,best_scaler,best_model,best_score,labels_\n",
    "  labels = None\n",
    "  model=DBSCAN(eps=0.5, min_samples=2)\n",
    "  labels=model.fit_predict(X)\n",
    "  # when cluster nuber is just 1, skip scoring\n",
    "  if len(pd.DataFrame(labels).drop_duplicates().to_numpy().flatten())>1:\n",
    "    score_result = silhouette_score(X, labels)\n",
    "    # if mean value of scores are bigger than max variable,\n",
    "    # update new options(model, scaler, k) to best options\n",
    "    if best_score < score_result:\n",
    "      best_score = score_result\n",
    "      best_scaler = cur_scaler\n",
    "      best_model = DBSCAN(eps=0.5, min_samples=2)\n",
    "      best_k_ = len(pd.DataFrame(labels).drop_duplicates().to_numpy().flatten())\n",
    "      labels_ = copy.deepcopy(labels)\n",
    "\n",
    "def AutoML(X,model_name,scalers,cluster_k):\n",
    "  global best_score,best_k_,best_scaler,best_model,best_score,labels_\n",
    "  cur_case = 1\n",
    "  total_case = len(scalers) * len(cluster_k)\n",
    "  #total_case = len(scalers) * len(models) * len(cluster_k)\n",
    "  # Find best scaler\n",
    "  for n in range(0, len(scalers)):\n",
    "    X = scalers[n].fit_transform(X)\n",
    "    for i in range(0,len(cluster_k)):\n",
    "      print(\"Progressing: (\",end=\"\")\n",
    "      print(cur_case,end=\"/\")\n",
    "      print(total_case,end=\")\\n\")\n",
    "      cur_case += 1\n",
    "      for i in range(len(model_name)):\n",
    "        if model_name[i]==\"KMeans\":\n",
    "          findKMeans(X,cluster_k[i],scalers[n])\n",
    "        elif model_name[i]==\"GMM\":\n",
    "          findGMM(X,cluster_k[i],scalers[n])\n",
    "        elif model_name[i]==\"CLARANS\":\n",
    "          findCLARANS(X,cluster_k[i],scalers[n])\n",
    "        elif model_name[i]==\"DBSCAN\":\n",
    "          findDBSCAN(X,scalers[n])\n",
    "        else:\n",
    "          print(\"no model\")\n",
    "\n",
    "\n",
    "# Import dataset\n",
    "base_src='./drive/MyDrive'\n",
    "df = pd.read_csv(base_src+\"/housing.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "# Drop useless feature\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "# LabelEncoder\n",
    "print(\"Encoder: LabelEncoder\")\n",
    "label_encoder = LabelEncoder()\n",
    "result = label_encoder.fit_transform(df['ocean_proximity'])\n",
    "df['ocean_proximity'] = result\n",
    "\n",
    "best_scaler = None\n",
    "best_model = None\n",
    "labels_ = None\n",
    "best_k_ = None\n",
    "best_score=-1.0\n",
    "\n",
    "#Median_house_value\n",
    "median_house_value=df['median_house_value']\n",
    "\n",
    "# Feature\n",
    "# Save origin dataframe to join clustered data after clustering\n",
    "dft = copy.deepcopy(df)\n",
    "print(\"Median_house_value:\")\n",
    "print(df['median_house_value'].describe())\n",
    "print(df.to_numpy())\n",
    "\n",
    "# Save origin dataframe to join clustered data after clustering\n",
    "dft = copy.deepcopy(df)\n",
    "print(\"Median_house_value:\")\n",
    "print(df['median_house_value'].describe())\n",
    "\n",
    "print(\"to_numpy: \", end=\"\")\n",
    "print(df.to_numpy())\n",
    "print(\"Columns: \", end=\"\")\n",
    "print(df.columns)\n",
    "\n",
    "#model_name=[\"KMeans\",\"GMM\",\"DBSCAN\",\"CLARANS\"]\n",
    "model_name=[\"KMeans\",\"GMM\",\"DBSCAN\"]\n",
    "AutoML(df,model_name,[StandardScaler(), RobustScaler(), MinMaxScaler(), MaxAbsScaler(),],range(2,12))\n",
    "\n",
    "print(\"\\nBest Scaler: \", end=\"\")\n",
    "print(best_scaler)\n",
    "print(\"Best Model: \", end=\"\")\n",
    "print(best_model)\n",
    "print(\"Score: \", end=\"\")\n",
    "print(best_score)\n",
    "print(\"labels: \", end=\"\")\n",
    "print(labels_)\n",
    "print(\"k: \", end=\"\")\n",
    "print(best_k_)\n",
    "\n",
    "\n",
    "# Analyze\n",
    "# Extrace cluster numbers \n",
    "cluster_info = pd.DataFrame(labels_).drop_duplicates().to_numpy().flatten()\n",
    "\n",
    "# Make dataframe for each cluster\n",
    "clusters_df = []\n",
    "for i in range(0, len(cluster_info)):\n",
    "  clusters_df.append(pd.DataFrame(columns=dft.columns))\n",
    "\n",
    "for i in range(0, len(labels_)):\n",
    "  clusters_df[labels_[i]] = clusters_df[labels_[i]].append(dft.iloc[i, :])\n",
    "\n",
    "print(\"Median_house_value:\")\n",
    "print(dft['median_house_value'].describe())\n",
    "\n",
    "# Print describe() to analyze clusters\n",
    "print(\"Cluster Info:\", cluster_info)\n",
    "for i in range(0, len(clusters_df)):\n",
    "  print(\"Cluster\", cluster_info[i])\n",
    "  print(clusters_df[i].describe())\n",
    "  print(\"\\n\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTcMobbbtNZb",
    "outputId": "2b9ff953-d233-40ea-86f3-f104c6d5a870"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoder: LabelEncoder\n",
      "Median_house_value:\n",
      "count     20433.000000\n",
      "mean     206864.413155\n",
      "std      115435.667099\n",
      "min       14999.000000\n",
      "25%      119500.000000\n",
      "50%      179700.000000\n",
      "75%      264700.000000\n",
      "max      500001.000000\n",
      "Name: median_house_value, dtype: float64\n",
      "[[-1.2223e+02  3.7880e+01  4.1000e+01 ...  8.3252e+00  4.5260e+05\n",
      "   3.0000e+00]\n",
      " [-1.2222e+02  3.7860e+01  2.1000e+01 ...  8.3014e+00  3.5850e+05\n",
      "   3.0000e+00]\n",
      " [-1.2224e+02  3.7850e+01  5.2000e+01 ...  7.2574e+00  3.5210e+05\n",
      "   3.0000e+00]\n",
      " ...\n",
      " [-1.2122e+02  3.9430e+01  1.7000e+01 ...  1.7000e+00  9.2300e+04\n",
      "   1.0000e+00]\n",
      " [-1.2132e+02  3.9430e+01  1.8000e+01 ...  1.8672e+00  8.4700e+04\n",
      "   1.0000e+00]\n",
      " [-1.2124e+02  3.9370e+01  1.6000e+01 ...  2.3886e+00  8.9400e+04\n",
      "   1.0000e+00]]\n",
      "Median_house_value:\n",
      "count     20433.000000\n",
      "mean     206864.413155\n",
      "std      115435.667099\n",
      "min       14999.000000\n",
      "25%      119500.000000\n",
      "50%      179700.000000\n",
      "75%      264700.000000\n",
      "max      500001.000000\n",
      "Name: median_house_value, dtype: float64\n",
      "to_numpy: [[-1.2223e+02  3.7880e+01  4.1000e+01 ...  8.3252e+00  4.5260e+05\n",
      "   3.0000e+00]\n",
      " [-1.2222e+02  3.7860e+01  2.1000e+01 ...  8.3014e+00  3.5850e+05\n",
      "   3.0000e+00]\n",
      " [-1.2224e+02  3.7850e+01  5.2000e+01 ...  7.2574e+00  3.5210e+05\n",
      "   3.0000e+00]\n",
      " ...\n",
      " [-1.2122e+02  3.9430e+01  1.7000e+01 ...  1.7000e+00  9.2300e+04\n",
      "   1.0000e+00]\n",
      " [-1.2132e+02  3.9430e+01  1.8000e+01 ...  1.8672e+00  8.4700e+04\n",
      "   1.0000e+00]\n",
      " [-1.2124e+02  3.9370e+01  1.6000e+01 ...  2.3886e+00  8.9400e+04\n",
      "   1.0000e+00]]\n",
      "Columns: Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
      "       'total_bedrooms', 'population', 'households', 'median_income',\n",
      "       'median_house_value', 'ocean_proximity'],\n",
      "      dtype='object')\n",
      "Progressing: (1/40)\n",
      "Progressing: (2/40)\n",
      "Progressing: (3/40)\n"
     ]
    }
   ]
  }
 ]
}
